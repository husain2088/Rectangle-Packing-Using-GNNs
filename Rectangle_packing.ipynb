{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Reward: -129.0\n",
      "Episode 11, Reward: 30.0\n",
      "Episode 21, Reward: 51.0\n",
      "Episode 31, Reward: -149.0\n",
      "Episode 41, Reward: 41.0\n",
      "Episode 51, Reward: -124.0\n",
      "Episode 61, Reward: -315.0\n",
      "Episode 71, Reward: -127.0\n",
      "Episode 81, Reward: -159.0\n",
      "Episode 91, Reward: -18.0\n",
      "Episode 101, Reward: -165.0\n",
      "Episode 111, Reward: -159.0\n",
      "Episode 121, Reward: -244.0\n",
      "Episode 131, Reward: -136.0\n",
      "Episode 141, Reward: -246.0\n",
      "Episode 151, Reward: -168.0\n",
      "Episode 161, Reward: -214.0\n",
      "Episode 171, Reward: -65.0\n",
      "Episode 181, Reward: -274.0\n",
      "Episode 191, Reward: -165.0\n",
      "Episode 201, Reward: -26.0\n",
      "Episode 211, Reward: -105.0\n",
      "Episode 221, Reward: 66.0\n",
      "Episode 231, Reward: -188.0\n",
      "Episode 241, Reward: 72.0\n",
      "Episode 251, Reward: -12.0\n",
      "Episode 261, Reward: -346.0\n",
      "Episode 271, Reward: -126.0\n",
      "Episode 281, Reward: -78.0\n",
      "Episode 291, Reward: -188.0\n",
      "Episode 301, Reward: -232.0\n",
      "Episode 311, Reward: -28.0\n",
      "Episode 321, Reward: -138.0\n",
      "Episode 331, Reward: -196.0\n",
      "Episode 341, Reward: -1.0\n",
      "Episode 351, Reward: -67.0\n",
      "Episode 361, Reward: -94.0\n",
      "Episode 371, Reward: -37.0\n",
      "Episode 381, Reward: -93.0\n",
      "Episode 391, Reward: -100.0\n",
      "Episode 401, Reward: -77.0\n",
      "Episode 411, Reward: 4.0\n",
      "Episode 421, Reward: -171.0\n",
      "Episode 431, Reward: 5.0\n",
      "Episode 441, Reward: -184.0\n",
      "Episode 451, Reward: -3.0\n",
      "Episode 461, Reward: -338.0\n",
      "Episode 471, Reward: -206.0\n",
      "Episode 481, Reward: -24.0\n",
      "Episode 491, Reward: -14.0\n",
      "Episode 501, Reward: -148.0\n",
      "Episode 511, Reward: -90.0\n",
      "Episode 521, Reward: -192.0\n",
      "Episode 531, Reward: -190.0\n",
      "Episode 541, Reward: -216.0\n",
      "Episode 551, Reward: -207.0\n",
      "Episode 561, Reward: 10.0\n",
      "Episode 571, Reward: -153.0\n",
      "Episode 581, Reward: 89.0\n",
      "Episode 591, Reward: -94.0\n",
      "Episode 601, Reward: -131.0\n",
      "Episode 611, Reward: -78.0\n",
      "Episode 621, Reward: -39.0\n",
      "Episode 631, Reward: -186.0\n",
      "Episode 641, Reward: -93.0\n",
      "Episode 651, Reward: -55.0\n",
      "Episode 661, Reward: -39.0\n",
      "Episode 671, Reward: -62.0\n",
      "Episode 681, Reward: 1.0\n",
      "Episode 691, Reward: 8.0\n",
      "Episode 701, Reward: -19.0\n",
      "Episode 711, Reward: -84.0\n",
      "Episode 721, Reward: -1.0\n",
      "Episode 731, Reward: -222.0\n",
      "Episode 741, Reward: -114.0\n",
      "Episode 751, Reward: -65.0\n",
      "Episode 761, Reward: -125.0\n",
      "Episode 771, Reward: -221.0\n",
      "Episode 781, Reward: -123.0\n",
      "Episode 791, Reward: 0.0\n",
      "Episode 801, Reward: -99.0\n",
      "Episode 811, Reward: -42.0\n",
      "Episode 821, Reward: -159.0\n",
      "Episode 831, Reward: -12.0\n",
      "Episode 841, Reward: 1.0\n",
      "Episode 851, Reward: -176.0\n",
      "Episode 861, Reward: -174.0\n",
      "Episode 871, Reward: -95.0\n",
      "Episode 881, Reward: -152.0\n",
      "Episode 891, Reward: -98.0\n",
      "Episode 901, Reward: 135.0\n",
      "Episode 911, Reward: -114.0\n",
      "Episode 921, Reward: 104.0\n",
      "Episode 931, Reward: -164.0\n",
      "Episode 941, Reward: -308.0\n",
      "Episode 951, Reward: -137.0\n",
      "Episode 961, Reward: -35.0\n",
      "Episode 971, Reward: -124.0\n",
      "Episode 981, Reward: -110.0\n",
      "Episode 991, Reward: -205.0\n",
      "Episode 1001, Reward: 39.0\n",
      "Episode 1011, Reward: 134.0\n",
      "Episode 1021, Reward: -107.0\n",
      "Episode 1031, Reward: -374.0\n",
      "Episode 1041, Reward: -111.0\n",
      "Episode 1051, Reward: -135.0\n",
      "Episode 1061, Reward: -191.0\n",
      "Episode 1071, Reward: 49.0\n",
      "Episode 1081, Reward: -163.0\n",
      "Episode 1091, Reward: 85.0\n",
      "Episode 1101, Reward: -107.0\n",
      "Episode 1111, Reward: -30.0\n",
      "Episode 1121, Reward: -15.0\n",
      "Episode 1131, Reward: -148.0\n",
      "Episode 1141, Reward: -91.0\n",
      "Episode 1151, Reward: -234.0\n",
      "Episode 1161, Reward: -121.0\n",
      "Episode 1171, Reward: -170.0\n",
      "Episode 1181, Reward: -158.0\n",
      "Episode 1191, Reward: -205.0\n",
      "Episode 1201, Reward: -170.0\n",
      "Episode 1211, Reward: -259.0\n",
      "Episode 1221, Reward: -110.0\n",
      "Episode 1231, Reward: -137.0\n",
      "Episode 1241, Reward: -71.0\n",
      "Episode 1251, Reward: -308.0\n",
      "Episode 1261, Reward: -179.0\n",
      "Episode 1271, Reward: -223.0\n",
      "Episode 1281, Reward: -19.0\n",
      "Episode 1291, Reward: 87.0\n",
      "Episode 1301, Reward: -130.0\n",
      "Episode 1311, Reward: 103.0\n",
      "Episode 1321, Reward: -29.0\n",
      "Episode 1331, Reward: -84.0\n",
      "Episode 1341, Reward: -8.0\n",
      "Episode 1351, Reward: 11.0\n",
      "Episode 1361, Reward: -89.0\n",
      "Episode 1371, Reward: 9.0\n",
      "Episode 1381, Reward: -62.0\n",
      "Episode 1391, Reward: -11.0\n",
      "Episode 1401, Reward: 23.0\n",
      "Episode 1411, Reward: -120.0\n",
      "Episode 1421, Reward: -227.0\n",
      "Episode 1431, Reward: -272.0\n",
      "Episode 1441, Reward: -22.0\n",
      "Episode 1451, Reward: -155.0\n",
      "Episode 1461, Reward: -94.0\n",
      "Episode 1471, Reward: -75.0\n",
      "Episode 1481, Reward: -302.0\n",
      "Episode 1491, Reward: -36.0\n",
      "Episode 1501, Reward: -204.0\n",
      "Episode 1511, Reward: -180.0\n",
      "Episode 1521, Reward: -140.0\n",
      "Episode 1531, Reward: -229.0\n",
      "Episode 1541, Reward: -129.0\n",
      "Episode 1551, Reward: 8.0\n",
      "Episode 1561, Reward: -89.0\n",
      "Episode 1571, Reward: 2.0\n",
      "Episode 1581, Reward: -52.0\n",
      "Episode 1591, Reward: -265.0\n",
      "Episode 1601, Reward: -102.0\n",
      "Episode 1611, Reward: -86.0\n",
      "Episode 1621, Reward: -312.0\n",
      "Episode 1631, Reward: -169.0\n",
      "Episode 1641, Reward: -197.0\n",
      "Episode 1651, Reward: -155.0\n",
      "Episode 1661, Reward: -32.0\n",
      "Episode 1671, Reward: -94.0\n",
      "Episode 1681, Reward: 88.0\n",
      "Episode 1691, Reward: -162.0\n",
      "Episode 1701, Reward: -231.0\n",
      "Episode 1711, Reward: -3.0\n",
      "Episode 1721, Reward: -77.0\n",
      "Episode 1731, Reward: -185.0\n",
      "Episode 1741, Reward: -10.0\n",
      "Episode 1751, Reward: -22.0\n",
      "Episode 1761, Reward: -159.0\n",
      "Episode 1771, Reward: -109.0\n",
      "Episode 1781, Reward: -182.0\n",
      "Episode 1791, Reward: -177.0\n",
      "Episode 1801, Reward: -84.0\n",
      "Episode 1811, Reward: -201.0\n",
      "Episode 1821, Reward: -205.0\n",
      "Episode 1831, Reward: -48.0\n",
      "Episode 1841, Reward: 15.0\n",
      "Episode 1851, Reward: -69.0\n",
      "Episode 1861, Reward: -61.0\n",
      "Episode 1871, Reward: -204.0\n",
      "Episode 1881, Reward: -35.0\n",
      "Episode 1891, Reward: -104.0\n",
      "Episode 1901, Reward: -327.0\n",
      "Episode 1911, Reward: 10.0\n",
      "Episode 1921, Reward: -49.0\n",
      "Episode 1931, Reward: -167.0\n",
      "Episode 1941, Reward: 8.0\n",
      "Episode 1951, Reward: -92.0\n",
      "Episode 1961, Reward: -105.0\n",
      "Episode 1971, Reward: 84.0\n",
      "Episode 1981, Reward: 2.0\n",
      "Episode 1991, Reward: -144.0\n",
      "Episode 2001, Reward: -55.0\n",
      "Episode 2011, Reward: -226.0\n",
      "Episode 2021, Reward: -49.0\n",
      "Episode 2031, Reward: -124.0\n",
      "Episode 2041, Reward: -47.0\n",
      "Episode 2051, Reward: -58.0\n",
      "Episode 2061, Reward: -93.0\n",
      "Episode 2071, Reward: 23.0\n",
      "Episode 2081, Reward: -199.0\n",
      "Episode 2091, Reward: -161.0\n",
      "Episode 2101, Reward: -68.0\n",
      "Episode 2111, Reward: -193.0\n",
      "Episode 2121, Reward: -322.0\n",
      "Episode 2131, Reward: -73.0\n",
      "Episode 2141, Reward: 160.0\n",
      "Episode 2151, Reward: -107.0\n",
      "Episode 2161, Reward: -158.0\n",
      "Episode 2171, Reward: 12.0\n",
      "Episode 2181, Reward: -156.0\n",
      "Episode 2191, Reward: -189.0\n",
      "Episode 2201, Reward: -92.0\n",
      "Episode 2211, Reward: -184.0\n",
      "Episode 2221, Reward: -139.0\n",
      "Episode 2231, Reward: -66.0\n",
      "Episode 2241, Reward: -40.0\n",
      "Episode 2251, Reward: 3.0\n",
      "Episode 2261, Reward: -11.0\n",
      "Episode 2271, Reward: 87.0\n",
      "Episode 2281, Reward: -62.0\n",
      "Episode 2291, Reward: -386.0\n",
      "Episode 2301, Reward: -63.0\n",
      "Episode 2311, Reward: -153.0\n",
      "Episode 2321, Reward: -251.0\n",
      "Episode 2331, Reward: -151.0\n",
      "Episode 2341, Reward: 4.0\n",
      "Episode 2351, Reward: -64.0\n",
      "Episode 2361, Reward: -177.0\n",
      "Episode 2371, Reward: 0.0\n",
      "Episode 2381, Reward: -95.0\n",
      "Episode 2391, Reward: -125.0\n",
      "Episode 2401, Reward: -173.0\n",
      "Episode 2411, Reward: 60.0\n",
      "Episode 2421, Reward: -149.0\n",
      "Episode 2431, Reward: 10.0\n",
      "Episode 2441, Reward: -110.0\n",
      "Episode 2451, Reward: -226.0\n",
      "Episode 2461, Reward: -113.0\n",
      "Episode 2471, Reward: -119.0\n",
      "Episode 2481, Reward: -85.0\n",
      "Episode 2491, Reward: -73.0\n",
      "Episode 2500, Reward: -64.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40, 0), (35, 36), (47, 10), (47, 2), (65, 7), (26, 25), (28, 31), (57, 21), (54, 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define rectangle dimensions (width, height)\n",
    "rectangles = [\n",
    "    (5, 3), (6, 4), (5, 2), (5, 5), (2, 6), (3, 4), (2, 2), (7, 4), (5, 5)\n",
    "]\n",
    "\n",
    "# Bin dimensions\n",
    "bin_width = 80\n",
    "bin_height = 40\n",
    "\n",
    "# Constraints for proximity\n",
    "proximity_constraints = [\n",
    "    (2, 3), (2, 4), (2, 8),  # Rectangle 3 close to 4, 5, and 9\n",
    "    (6, 5), (6, 1)  # Rectangle 7 close to 6 and 2\n",
    "]\n",
    "\n",
    "# Graph Neural Network model for encoding relationships between rectangles\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network (GNN) for encoding relationships between rectangles.\n",
    "    This model uses two graph convolutional layers to process node features and edge relationships.\n",
    "    Attributes:\n",
    "        conv1 (GCNConv): The first graph convolutional layer.\n",
    "        conv2 (GCNConv): The second graph convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GNN model with input, hidden, and output dimensions.\n",
    "        Args:\n",
    "            input_dim (int): The dimensionality of the input features.\n",
    "            hidden_dim (int): The dimensionality of the hidden layer.\n",
    "            output_dim (int): The dimensionality of the output features.\n",
    "        \"\"\"\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = torch_geometric.nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = torch_geometric.nn.GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the GNN model.\n",
    "        Args:\n",
    "            data (torch_geometric.data.Data): The graph data containing node features (x) and edge indices (edge_index).\n",
    "        Returns:\n",
    "            torch.Tensor: The output node features after processing through the GNN.\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Reinforcement Learning agent\n",
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    Reinforcement Learning agent for optimizing rectangle placements.\n",
    "    Attributes:\n",
    "        gnn (torch.nn.Module): Graph Neural Network model for state encoding.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training the GNN.\n",
    "        best_reward (float): Best reward achieved so far.\n",
    "        best_placements (list): Placements corresponding to the best reward.\n",
    "        exploration_penalty (float): Penalty applied to encourage exploration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gnn):\n",
    "        \"\"\"\n",
    "        Initialize the RL agent.\n",
    "        Args:\n",
    "            gnn (torch.nn.Module): Graph Neural Network model for encoding states.\n",
    "        \"\"\"\n",
    "        self.gnn = gnn\n",
    "        self.optimizer = optim.Adam(self.gnn.parameters(), lr=0.01)\n",
    "        self.best_reward = float('-inf')  # Initialize with a very low value\n",
    "        self.best_placements = None  # To store the best placements\n",
    "        self.exploration_penalty = 0.025  # Small penalty to discourage random exploration once we find good solutions\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Predict actions for a given state using the GNN.\n",
    "        Args:\n",
    "            state (torch_geometric.data.Data): Encoded graph state.\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted actions for the rectangles.\n",
    "        \"\"\"\n",
    "        # Predict action from state using GNN\n",
    "        return self.gnn(state)\n",
    "    \n",
    "    def update(self, state, reward, placements):\n",
    "        \"\"\"\n",
    "        Update the GNN model based on reward and placements.\n",
    "        Args:\n",
    "            state (torch_geometric.data.Data): Current graph state.\n",
    "            reward (float): Reward achieved for the current placements.\n",
    "            placements (list): Current placements of rectangles.\n",
    "        \"\"\"\n",
    "        # Track the best reward and corresponding placements\n",
    "        if reward > self.best_reward:\n",
    "            self.best_reward = reward  # Update best reward if current reward is higher\n",
    "            self.best_placements = placements  # Store the best placements\n",
    "        \n",
    "        # Apply penalty to encourage finding better solutions and refining the best found ones\n",
    "        if reward == self.best_reward:\n",
    "            reward -= self.exploration_penalty  # Penalty for exploration around best solution (discourage randomness)\n",
    "        \n",
    "        # Convert reward to a tensor for backpropagation\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # We want to maximize the reward, so negate the reward to treat it as a loss\n",
    "        loss = -reward_tensor  # Negative reward for maximization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()  # Compute gradients\n",
    "        self.optimizer.step()  # Update model parameters\n",
    "\n",
    "# Function to encode state of the bin (rectangles, positions, constraints)\n",
    "def encode_state(rectangles, bin_width, bin_height, proximity_constraints):\n",
    "    \"\"\"\n",
    "    Encode the state of the bin into a graph representation.\n",
    "    Args:\n",
    "        rectangles (list): List of rectangle dimensions as (width, height).\n",
    "        bin_width (int): Width of the bin.\n",
    "        bin_height (int): Height of the bin.\n",
    "        proximity_constraints (list): List of pairs of rectangles that must be close.\n",
    "    Returns:\n",
    "        Data: A `torch_geometric.data.Data` object containing node features and edge indices.\n",
    "    \"\"\"\n",
    "    num_rectangles = len(rectangles)\n",
    "    \n",
    "    # Node features (width, height, and proximity)\n",
    "    node_features = torch.tensor([[w, h] for w, h in rectangles], dtype=torch.float)\n",
    "    \n",
    "    # Edge index (pairs of rectangles that are close to each other)\n",
    "    edge_index = []\n",
    "    for (i, j) in proximity_constraints:\n",
    "        edge_index.append([i, j])\n",
    "        edge_index.append([j, i])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# Function to check if two rectangles overlap, considering 1-unit separation\n",
    "def is_overlap(rect1, rect2, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Check if two rectangles overlap, considering a 1-unit separation.\n",
    "\n",
    "    :param rect1: A tuple representing the width and height of the first rectangle.\n",
    "    :param rect2: A tuple representing the width and height of the second rectangle.\n",
    "    :param x1: The x-coordinate of the bottom-left corner of the first rectangle.\n",
    "    :param y1: The y-coordinate of the bottom-left corner of the first rectangle.\n",
    "    :param x2: The x-coordinate of the bottom-left corner of the second rectangle.\n",
    "    :param y2: The y-coordinate of the bottom-left corner of the second rectangle.\n",
    "\n",
    "    :return: True if the rectangles overlap, False otherwise.\n",
    "    \"\"\"\n",
    "    w1, h1 = rect1\n",
    "    w2, h2 = rect2\n",
    "    \n",
    "    # Adding 1-unit separation to the overlap condition\n",
    "    return not (x1 + w1 + 1 <= x2 or x1 >= x2 + w2 + 1 or y1 + h1 + 1 <= y2 or y1 >= y2 + h2 + 1)\n",
    "\n",
    "# Function to place the rectangles based on GNN-guided reinforcement learning\n",
    "def place_rectangles(rectangles, bin_width, bin_height, proximity_constraints, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Place rectangles in a bin using reinforcement learning with a Graph Neural Network (GNN) agent.\n",
    "\n",
    "    :param rectangles: A list of tuples representing the dimensions (width, height) of the rectangles.\n",
    "    :param bin_width: The width of the bin.\n",
    "    :param bin_height: The height of the bin.\n",
    "    :param proximity_constraints: Constraints for maintaining minimum separation between rectangles.\n",
    "    :param num_episodes: The number of episodes for training the agent (default is 100).\n",
    "\n",
    "    :return: The best placements of rectangles corresponding to the highest reward found by the agent.\n",
    "    \"\"\"\n",
    "    # Initialize GNN and agent\n",
    "    input_dim = 2  # width and height\n",
    "    hidden_dim = 16\n",
    "    output_dim = 2  # x, y position\n",
    "    gnn = GNN(input_dim, hidden_dim, output_dim)\n",
    "    agent = RLAgent(gnn)\n",
    "    \n",
    "    placements = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Encode the current state of the bin\n",
    "        state = encode_state(rectangles, bin_width, bin_height, proximity_constraints)\n",
    "        \n",
    "        # Get action (placement for each rectangle)\n",
    "        predicted_placements = agent.get_action(state).detach().numpy()  # Ensure it's a NumPy array\n",
    "        \n",
    "        # Try to place the rectangles one by one\n",
    "        valid_placements = []\n",
    "        \n",
    "        # Place Rectangle 1 on top of the bin (y=0)\n",
    "        rect1_x = random.randint(0, bin_width - rectangles[0][0])\n",
    "        valid_placements.append((rect1_x, 0))\n",
    "        \n",
    "        # Place Rectangle 2 on the bottom of the bin (y=bin_height - height of rectangle 2)\n",
    "        rect2_x = random.randint(0, bin_width - rectangles[1][0])\n",
    "        valid_placements.append((rect2_x, bin_height - rectangles[1][1]))\n",
    "        \n",
    "        # Place the remaining rectangles\n",
    "        for i, (x, y) in enumerate(predicted_placements):\n",
    "            if i not in [0, 1]:  # Skip the first two rectangles\n",
    "                # Try to place the rectangle at (x, y)\n",
    "                if 0 <= x <= bin_width - rectangles[i][0] and 0 <= y <= bin_height - rectangles[i][1]:\n",
    "                    # Check for overlap with previously placed rectangles\n",
    "                    overlap = False\n",
    "                    for (px, py) in valid_placements:\n",
    "                        if is_overlap(rectangles[i], rectangles[i], px, py, x, y):\n",
    "                            overlap = True\n",
    "                            break\n",
    "                    \n",
    "                    if not overlap:\n",
    "                        valid_placements.append((x, y))\n",
    "                    else:\n",
    "                        # If overlap, try again with different coordinates (for simplicity, random adjustment here)\n",
    "                        found_valid_position = False\n",
    "                        while not found_valid_position:\n",
    "                            x_new = random.randint(0, bin_width - rectangles[i][0])\n",
    "                            y_new = random.randint(0, bin_height - rectangles[i][1])\n",
    "                            overlap = False\n",
    "                            for (px, py) in valid_placements:\n",
    "                                if is_overlap(rectangles[i], rectangles[i], px, py, x_new, y_new):\n",
    "                                    overlap = True\n",
    "                                    break\n",
    "                            if not overlap:\n",
    "                                valid_placements.append((x_new, y_new))\n",
    "                                found_valid_position = True\n",
    "                else:\n",
    "                    # If outside bin, try again with different coordinates\n",
    "                    found_valid_position = False\n",
    "                    while not found_valid_position:\n",
    "                        x_new = random.randint(0, bin_width - rectangles[i][0])\n",
    "                        y_new = random.randint(0, bin_height - rectangles[i][1])\n",
    "                        if 0 <= x_new <= bin_width - rectangles[i][0] and 0 <= y_new <= bin_height - rectangles[i][1]:\n",
    "                            overlap = False\n",
    "                            for (px, py) in valid_placements:\n",
    "                                if is_overlap(rectangles[i], rectangles[i], px, py, x_new, y_new):\n",
    "                                    overlap = True\n",
    "                                    break\n",
    "                            if not overlap:\n",
    "                                valid_placements.append((x_new, y_new))\n",
    "                                found_valid_position = True\n",
    "        \n",
    "        placements = valid_placements\n",
    "        reward = calculate_reward(placements, bin_width, bin_height, proximity_constraints, rectangles)\n",
    "        agent.update(state, reward, placements)\n",
    "\n",
    "        if episode%10 == 0 or episode == 2499:\n",
    "            print(f\"Episode {episode+1}, Reward: {reward}\")\n",
    "    \n",
    "    return agent.best_placements  # Return the best placements corresponding to the best reward\n",
    "\n",
    "# Function to calculate the reward for a given placement\n",
    "def calculate_reward(placements, bin_width, bin_height, proximity_constraints, rectangles):\n",
    "    \"\"\"\n",
    "    Calculate the reward for a given placement of rectangles in a bin.\n",
    "\n",
    "    :param placements: A list of tuples representing the (x, y) positions of the placed rectangles.\n",
    "    :param bin_width: The width of the bin.\n",
    "    :param bin_height: The height of the bin.\n",
    "    :param proximity_constraints: A list of tuple pairs specifying which rectangles should be close to each other.\n",
    "    :param rectangles: A list of tuples representing the dimensions (width, height) of the rectangles.\n",
    "\n",
    "    :return: The calculated reward based on the placement, area coverage, and proximity penalties.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    \n",
    "    # Calculate area covered (subtract empty space)\n",
    "    for (x, y) in placements:\n",
    "        if not 0 <= x <= bin_width and 0 <= y <= bin_height:\n",
    "            reward -= 100  # Simple area reward (each valid placement increases reward)\n",
    "    \n",
    "    # Add proximity penalties based on constraints\n",
    "    for (i, j) in proximity_constraints:\n",
    "        x1, y1 = placements[i]\n",
    "        x2, y2 = placements[j]\n",
    "        \n",
    "        # Compute the closest corners between two rectangles\n",
    "        w1, h1 = rectangles[i]\n",
    "        w2, h2 = rectangles[j]\n",
    "        \n",
    "        # Corners of both rectangles\n",
    "        corners_i = [(x1, y1), (x1 + w1, y1), (x1, y1 + h1), (x1 + w1, y1 + h1)]\n",
    "        corners_j = [(x2, y2), (x2 + w2, y2), (x2, y2 + h2), (x2 + w2, y2 + h2)]\n",
    "        \n",
    "        min_distance = float('inf')  # Initialize to a large number\n",
    "        \n",
    "        # Find the closest corners\n",
    "        for corner_i in corners_i:\n",
    "            for corner_j in corners_j:\n",
    "                dist = abs(corner_i[0] - corner_j[0]) + abs(corner_i[1] - corner_j[1])\n",
    "                min_distance = min(min_distance, dist)\n",
    "        \n",
    "        # Apply a penalty if the minimum distance is greater than 10\n",
    "        if min_distance < 25:\n",
    "            reward += 50  # Penalize for not being close enough\n",
    "\n",
    "        # Manhattan distance between centers of rectangles i and j\n",
    "        center1_x, center1_y = x1 + w1 / 2, y1 + h1 / 2\n",
    "        center2_x, center2_y = x2 + w2 / 2, y2 + h2 / 2\n",
    "        \n",
    "        # Manhattan distance between the centers\n",
    "        manhattan_distance = abs(center1_x - center2_x) + abs(center1_y - center2_y)\n",
    "        \n",
    "        # Reward for reducing Manhattan distance (the smaller, the better)\n",
    "        reward -= manhattan_distance  # This will reduce the reward if the distance is large\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Plot the final bin with placements and annotate each rectangle\n",
    "def plot_bin(bin_width, bin_height, rectangles, placements):\n",
    "    \"\"\"\n",
    "    Plot the bin with rectangle placements and annotate each rectangle.\n",
    "\n",
    "    :param bin_width: The width of the bin.\n",
    "    :param bin_height: The height of the bin.\n",
    "    :param rectangles: A list of tuples representing the dimensions (width, height) of the rectangles.\n",
    "    :param placements: A list of tuples representing the (x, y) positions of the placed rectangles.\n",
    "\n",
    "    :return: None. Displays the plot of the bin with annotated rectangle placements.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.set_xlim(0, bin_width)\n",
    "    ax.set_ylim(0, bin_height)\n",
    "\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        x, y = placements[i]  # No need to detach; already in NumPy format\n",
    "        if x is not None and y is not None:\n",
    "            ax.add_patch(plt.Rectangle((x, y), rect[0], rect[1], edgecolor='black', facecolor='blue', fill=True))\n",
    "            ax.text(x + rect[0]/2, y + rect[1]/2, str(i+1), color='white', ha='center', va='center', fontsize=12)\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')  # Square aspect ratio\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to run RL-based rectangle placement and plot the best result.\n",
    "\n",
    "    This function runs the rectangle placement optimization using reinforcement learning (RL),\n",
    "    then plots the best placements of the rectangles inside the bin with annotations.\n",
    "\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    # Run RL-based rectangle placement\n",
    "    best_placements = place_rectangles(rectangles, bin_width, bin_height, proximity_constraints, num_episodes=2500)\n",
    "    \n",
    "    # Plot the best result with annotations\n",
    "    plot_bin(bin_width, bin_height, rectangles, best_placements)\n",
    "    print(best_placements)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
